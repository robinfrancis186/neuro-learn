# NeuroLearn AI ğŸ§ âœ¨

> **Learning Without Limits â€” One Story at a Time**

NeuroLearn AI is an adaptive, emotion-aware educational platform specifically designed for neurodivergent students. It combines AI-powered story generation, real-time emotion detection, and personalized learning experiences to create an inclusive learning environment.

## ğŸŒŸ Features

### Core Learning Platform
- **ğŸ­ Emotion Engine**: Real-time facial emotion detection using Google ML Kit
- **ğŸ“š AI Story Generation**: Powered by Mistral 7B and adaptive storytelling
- **ğŸ—£ï¸ Communication Tools**: Visual boards, emotion expression, and social skills practice
- **ğŸ¯ Personalized Learning**: Adaptive content based on cognitive profiles and emotional states
- **ğŸ“Š Analytics Dashboard**: Comprehensive progress tracking and insights
- **ğŸ® Interactive Elements**: Flashcards, AR geography, and gamified learning

### Accessibility & Neurodivergence Support
- **â™¿ Inclusive Design**: High contrast modes, reduced animations, and sensory preferences
- **ğŸ§© Cognitive Profiling**: Attention span tracking, learning style adaptation
- **ğŸ’¬ Communication Assistance**: Alternative communication methods and visual supports
- **ğŸ¨ Mood-Based Content**: Stories adapt to emotional states for optimal learning
- **â±ï¸ Break Management**: Automatic break suggestions based on attention patterns

## ğŸ—ï¸ Architecture

### Frontend (Flutter Web)
```
lib/
â”œâ”€â”€ core/                     # Core business logic and models
â”‚   â”œâ”€â”€ models/              # Data models (Student, EmotionalState, etc.)
â”‚   â”œâ”€â”€ services/            # Business logic services
â”‚   â””â”€â”€ utils/               # Utility functions and helpers
â”œâ”€â”€ features/                # Feature-based modules
â”‚   â”œâ”€â”€ emotion_engine/      # Emotion detection and analysis
â”‚   â”œâ”€â”€ story_tutor/         # Story generation and management
â”‚   â”œâ”€â”€ communication/       # Communication assistance tools
â”‚   â”œâ”€â”€ dashboard/           # Analytics and progress tracking
â”‚   â”œâ”€â”€ flashcards/          # Interactive learning cards
â”‚   â””â”€â”€ ar_geography/        # Augmented reality learning
â”œâ”€â”€ shared/                  # Shared UI components and themes
â”‚   â”œâ”€â”€ widgets/             # Reusable UI components
â”‚   â”œâ”€â”€ themes/              # App theming and styling
â”‚   â””â”€â”€ providers/           # State management (Riverpod)
â””â”€â”€ main.dart               # Application entry point
```

### Backend (Python/FastAPI)
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                 # API endpoints
â”‚   â”œâ”€â”€ core/                # Core backend logic
â”‚   â”œâ”€â”€ models/              # Database models
â”‚   â””â”€â”€ services/            # Business logic services
â”œâ”€â”€ mistral_integration/     # Mistral AI integration
â”œâ”€â”€ emotion_processing/      # Emotion data processing
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites
- Flutter SDK (>=3.0.0)
- Dart SDK (>=3.0.0)
- Python 3.11 (required for backend)
- Chrome/Edge browser (for web development)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-org/neurolearn-ai.git
   cd neurolearn-ai
   ```

2. **Install Flutter dependencies**
   ```bash
   flutter pub get
   ```

3. **Set up backend environment**
   ```bash
   # Make setup script executable
   chmod +x setup.sh
   
   # Run setup script (creates Python 3.11 virtual environment and installs dependencies)
   ./setup.sh
   
   # Activate virtual environment
   source backend/venv/bin/activate
   ```

4. **Run the application**
   ```bash
   # Frontend (Flutter Web)
   flutter run -d web-server --web-port=8087
   
   # Backend (Optional - for full AI features)
   cd backend
   python -m uvicorn app.main:app --reload
   ```

5. **Open in browser**
   Navigate to `http://localhost:8087`

### Development Environment

The project uses Python 3.11 for the backend. The `setup.sh` script will:
- Check if Python 3.11 is installed
- Create a virtual environment using Python 3.11
- Install all required dependencies
- Set up the correct environment for development

If Python 3.11 is not installed, you can install it using:
```bash
# On macOS with Homebrew
brew install python@3.11

# On Ubuntu/Debian
sudo apt update
sudo apt install python3.11 python3.11-venv

# On Windows
# Download Python 3.11 installer from python.org
```

## ğŸ¯ Core Features Deep Dive

### Emotion Engine
The Emotion Engine uses advanced facial recognition to detect student emotions and adapt content accordingly:

- **Real-time Detection**: 2-second intervals with confidence scoring
- **8 Emotion Categories**: Happiness, sadness, anger, fear, surprise, disgust, contempt, neutral
- **Mood Mapping**: Emotions mapped to 7 story mood types for optimal learning
- **Intervention System**: Automatic calming or energizing content recommendations

### Story Generation System
AI-powered storytelling that creates personalized educational content:

- **Memory Integration**: Family stories converted to learning adventures
- **Adaptive Narrative**: Stories adjust based on emotional state and learning progress
- **Multi-modal Input**: Voice notes, manual entry, and guided story building
- **Educational Focus**: Math, reading, science concepts woven into engaging narratives

### Communication Tools
Comprehensive communication assistance for neurodivergent learners:

- **Visual Communication Board**: Category-based word selection
- **Emotion Expression**: Visual emotion identification and communication
- **Social Skills Practice**: Interactive conversation scenarios
- **Voice Practice**: Speech exercises with feedback

## ğŸ› ï¸ Development

### Code Style & Standards
- **Dart/Flutter**: Follow official Dart style guide
- **File Organization**: Feature-based folder structure
- **Naming Conventions**: PascalCase for classes, camelCase for variables
- **Documentation**: Comprehensive inline documentation for all public APIs

### State Management
Using **Riverpod** for predictable state management:
- Providers for global state
- StateNotifiers for complex state logic
- Consumer widgets for reactive UI

### Testing Strategy
```bash
# Unit tests
flutter test

# Integration tests
flutter test integration_test/

# Widget tests
flutter test test/widget_test/
```

### Adding New Features

1. **Create feature module** in `lib/features/`
2. **Define models** in `lib/core/models/`
3. **Implement services** in `lib/core/services/`
4. **Add routing** in main navigation
5. **Write tests** for all components

## ğŸ“± Supported Platforms

- âœ… **Web** (Primary platform)
- ğŸ”„ **iOS** (Planned)
- ğŸ”„ **Android** (Planned)
- ğŸ”„ **Desktop** (Future consideration)

## ğŸ”§ Configuration

### Environment Variables
Create `.env` file for configuration:
```bash
# AI Model Configuration
MISTRAL_API_KEY=your_mistral_key
OPENAI_API_KEY=your_openai_key

# Database Configuration
DATABASE_URL=your_database_url

# Feature Flags
ENABLE_EMOTION_ENGINE=true
ENABLE_AR_FEATURES=false
```

### ML Kit Setup
For emotion detection, ensure ML Kit dependencies are properly configured:
```yaml
dependencies:
  google_ml_kit: ^0.15.0
  camera: ^0.10.0
  permission_handler: ^10.0.0
```

### Model Context Protocol (MCP) Integration
NeuroLearn AI includes Context7 MCP server for enhanced development experience:

- **Real-time Documentation**: Instant access to up-to-date library documentation
- **AI-Assisted Development**: Smart code completion with current best practices
- **Framework Support**: Flutter, Dart, Python, FastAPI, and ML libraries

**Configuration**: `~/.cursor/mcp.json`
```json
{
  "mcpServers": {
    "context7": {
      "command": "npx",
      "args": ["-y", "@upstash/context7-mcp@latest"]
    }
  }
}
```

See [MCP Integration Guide](docs/mcp_integration.md) for detailed setup and usage instructions.

## ğŸ¨ Theming & Accessibility

### Design System
- **Primary Colors**: Blue (#2196F3) for focus, Green (#4CAF50) for comfort
- **Typography**: Accessible fonts with clear hierarchy
- **Spacing**: 8px grid system for consistent layouts
- **Components**: Material Design 3 with custom neurodivergent adaptations

### Accessibility Features
- High contrast mode
- Reduced motion options
- Large text support
- Screen reader compatibility
- Keyboard navigation

## ğŸ“Š Analytics & Privacy

### Data Collection
- **Learning Progress**: Anonymized learning analytics
- **Emotion Data**: Local processing, no cloud storage
- **Usage Patterns**: Aggregate usage statistics for improvement

### Privacy Compliance
- COPPA compliant for children's data
- GDPR ready with data export/deletion
- Local-first architecture for sensitive data

## ğŸ¤ Contributing

### Getting Started
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### Development Workflow
1. **Issue Creation**: Use GitHub issues for bug reports and feature requests
2. **Code Review**: All PRs require review and automated testing
3. **Testing**: Maintain >80% code coverage
4. **Documentation**: Update docs for any API changes

## ğŸ“‹ Roadmap

### Phase 1 (Current)
- âœ… Core emotion detection
- âœ… Basic story generation
- âœ… Communication tools
- âœ… Web platform

### Phase 2 (Next 3 months)
- ğŸ”„ Advanced AI tutoring
- ğŸ”„ Parent/teacher dashboard
- ğŸ”„ Mobile app development
- ğŸ”„ Multi-language support

### Phase 3 (6-12 months)
- ğŸ”„ AR/VR learning experiences
- ğŸ”„ Advanced analytics
- ğŸ”„ Third-party integrations
- ğŸ”„ Marketplace for content

## ğŸ“š Documentation

### Core Documentation
- [ğŸ“– Development Guide](DEVELOPMENT_GUIDE.md) - Comprehensive development setup and guidelines
- [ğŸ—ï¸ Architecture](docs/architecture.md) - System architecture and design patterns
- [ğŸ“ Code Organization](docs/code_organization.md) - Project structure and file organization
- [ğŸš€ Deployment Guide](docs/deployment.md) - Deployment strategies and configurations

### API & Technical Reference
- [ğŸ”Œ API Reference](docs/api.md) - Complete API documentation with examples
- [ğŸ¨ Widget Catalog](docs/widgets.md) - UI component library and usage guide
- [ğŸ”§ Troubleshooting](docs/troubleshooting.md) - Common issues and solutions

### Specialized Guides
- [ğŸ§© Neurodivergence Support](docs/neurodivergence.md) - Understanding and supporting neurodivergent learners
- [â™¿ Accessibility Guidelines](docs/accessibility/) - WCAG compliance and inclusive design
- [ğŸ¯ Testing Strategy](docs/testing.md) - Comprehensive testing approaches

## ğŸ†˜ Support & Resources

### Getting Help
- **ğŸ“‹ GitHub Issues**: [Report bugs and request features](https://github.com/your-org/neurolearn-ai/issues)
- **ğŸ’¬ Discord Community**: [Join our developer community](https://discord.gg/neurolearn-ai)
- **ğŸ“§ Email Support**: technical-support@neurolearn-ai.com
- **ğŸ“– Documentation**: [Browse comprehensive guides](docs/)

### Community Guidelines
- **ğŸ¤ Be Respectful**: Treat all contributors with kindness and respect
- **ğŸŒŸ Be Inclusive**: Consider neurodivergent users in all design decisions
- **ğŸ’¡ Be Constructive**: Provide helpful feedback and actionable suggestions
- **ğŸ¯ Be Patient**: Remember we're all learning and growing together

### Educational Resources
- [Understanding Neurodivergence](docs/neurodivergence.md) - Research-backed insights
- [Inclusive Design Principles](docs/accessibility/guidelines.md) - Best practices for accessibility
- [Emotion Recognition Ethics](docs/ethics.md) - Ethical considerations and privacy

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Google ML Kit team for emotion detection capabilities
- Mistral AI for advanced language modeling
- Flutter team for the amazing cross-platform framework
- Neurodivergent community for guidance and feedback

---

**Made with â¤ï¸ for inclusive education**

*NeuroLearn AI - Empowering every learner to reach their full potential* 